{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from scipy import stats\n",
    "import uproot\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Creating the noise file \n",
    "def open_noise(filename) :\n",
    "    noise0=uproot.open(filename) #using uproot to open the root file \n",
    "    noise1=noise0['pixTree']\n",
    "    noise=pd.DataFrame(noise1['fData'].array(library=\"np\"))\n",
    "    noise=noise[noise.duplicated(subset=['row','col'])==True]\n",
    "    noise.drop_duplicates(subset=['row','col'],inplace=True)  #one consider a pixel tobe noisy xhen on a short run (<1minute), with no source it declares a hit more than one time.\n",
    "    X_noise=np.array(noise.col+((noise.chipId-4)*1024)) #changing the coordinates \n",
    "    Y_noise=np.array(noise.row)\n",
    "    return X_noise,Y_noise\n",
    "\n",
    "#put in the Noise data \n",
    "Noise1=open_noise('../Noise1')\n",
    "Noise2=open_noise('../Noise2')\n",
    "Noise3=open_noise('../Noise3')\n",
    "Noise4=open_noise('../Noise4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the root file and getting the tree\n",
    "def open_file (filename,id):\n",
    "    file=uproot.open(filename)\n",
    "    file1=file['pixTree']\n",
    "    Data=file1['fData'].array(library=\"np\")\n",
    "    data=pd.DataFrame(Data)\n",
    "    data['ldr']=id\n",
    "    data.col=((data.chipId-4)*1024)+data.col\n",
    "    return data\n",
    "\n",
    "data=open_file(\"../Data1\")\n",
    "data2=open_file(\"../Data2\")\n",
    "data3=open_file(\"../Data3\")\n",
    "data4=open_file(\"../Data4\")\n",
    "\n",
    "data=pd.concat([data,data2])\n",
    "data=pd.concat([data,data3])\n",
    "data=pd.concat([data,data4])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(4)\n",
    "\n",
    "display(data) #show the table with the different entries\n",
    "axes[0].hist2d(data[data.ldr==1].col,data[data.ldr==1].row,bins=100,density=True) #plotting the data as a 2D histogram (at this point data from ladder 2 and 4 are flipped)\n",
    "axes[1].hist2d(data[data.ldr==2].col,data[data.ldr==2].row,bins=100,density=True)\n",
    "axes[2].hist2d(data[data.ldr==3].col,data[data.ldr==3].row,bins=100,density=True)\n",
    "axes[3].hist2d(data[data.ldr==4].col,data[data.ldr==4].row,bins=100,density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove noisy pixels \n",
    "def remove_noise (noise,data) :\n",
    "        n_0=data[data.col.isin(noise[0]) & data.row.isin(noise[1])].index\n",
    "        if len(n_0)>0:\n",
    "            data.drop(index=n_0,inplace=True)\n",
    "        return data \n",
    "\n",
    "\n",
    "data_filtered=remove_noise(Noise1,data[data.ldr==1])\n",
    "data_filtered=pd.concat([data_filtered,remove_noise(Noise2,data[data.ldr==2])])\n",
    "data_filtered=pd.concat([data_filtered,remove_noise(Noise2,data[data.ldr==3])])\n",
    "data_filtered=pd.concat([data_filtered,remove_noise(Noise2,data[data.ldr==4])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Displaying the filtered data (Monitoring the experiment)\n",
    "display(data_filtered)\n",
    "fig, axes = plt.subplots(4)\n",
    "axes[0].hist2d(data_filtered[(data_filtered.ldr==1)&(data_filtered.chipId==8)].col,data_filtered[(data_filtered.ldr==1)&(data_filtered.chipId==8)].row,bins=[600,300],norm='symlog')\n",
    "axes[1].hist2d(data_filtered[(data_filtered.ldr==2)&(data_filtered.chipId==8)].col,data_filtered[(data_filtered.ldr==2)&(data_filtered.chipId==8)].row,bins=[600,300],norm='symlog')\n",
    "axes[2].hist2d(data_filtered[(data_filtered.ldr==3)&(data_filtered.chipId==8)].col,data_filtered[(data_filtered.ldr==3)&(data_filtered.chipId==8)].row,bins=[600,300],norm='symlog')\n",
    "axes[3].hist2d(data_filtered[(data_filtered.ldr==4)&(data_filtered.chipId==8)].col,data_filtered[(data_filtered.ldr==4)&(data_filtered.chipId==8)].row,bins=[600,300],norm='symlog')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flipping the data from the even plane in y \n",
    "def flip_data_row (data) :\n",
    "    data.row=512-data.row\n",
    "    return data\n",
    "\n",
    "data_filtered[(data_filtered.ldr==2)]=flip_data_row(data_filtered[(data_filtered.ldr==2)])\n",
    "data_filtered[(data_filtered.ldr==4)]=flip_data_row(data_filtered[(data_filtered.ldr==4)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alignement of the telescope \n",
    "def track (Pos_1,Pos_2,z) :\n",
    "    x=((z-Pos_1[2])/(Pos_2[2]-Pos_1[2]))+Pos_1[0]\n",
    "    y=((z-Pos_1[2])/(Pos_2[2]-Pos_1[2]))+Pos_1[1]\n",
    "    return [x,y]\n",
    "\n",
    "#for this first alignement, we just consider the mean in x and y of the hits and we built track 2 by 2, then moving the other plane to be on this track\n",
    "def alignement_telescope (DATA,Z) :\n",
    "    for i in range (max(DATA.ldr)) :\n",
    "        for j in range (max(DATA.ldr)-1,i,-1) :\n",
    "            for k in range (max(DATA.ldr)) :\n",
    "                if k!=j and k!=i :\n",
    "                    Moved=track([np.mean(DATA[DATA.ldr==i+1].col),np.mean(DATA[DATA.ldr==i+1].row),Z[i]],[np.mean(DATA[DATA.ldr==j+1].col),np.mean(DATA[DATA.ldr==j+1].row),Z[j]],Z[k])\n",
    "                    DATA.loc[DATA.ldr==k+1,'row']=DATA[DATA.ldr==k+1].row+(Moved[1]-np.mean(DATA[DATA.ldr==k+1].row))\n",
    "                    DATA.loc[DATA.ldr==k+1,'col']=DATA[DATA.ldr==k+1].col+(Moved[0]-np.mean(DATA[DATA.ldr==k+1].col))\n",
    "    return DATA\n",
    "\n",
    "\n",
    "Z=[0,10.6,145,155.6]#coordinates of the differents planes of BANCO\n",
    "data_aligned=alignement_telescope(data_filtered[data_filtered.chipId==8],Z)\n",
    "display(data_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting superposed (to see if planes are aligned)\n",
    "fig, axes = plt.subplots()\n",
    "plt.hist2d(data_aligned[data_aligned.ldr==1].col,data_aligned[data_aligned.ldr==1].row,bins=[600,300],norm='symlog',cmap='Greens')\n",
    "plt.hist2d(data_aligned[data_aligned.ldr==2].col,data_aligned[data_aligned.ldr==2].row,bins=[600,300],norm='symlog',cmap='Blues',alpha=0.2)\n",
    "plt.hist2d(data_aligned[data_aligned.ldr==3].col,data_aligned[data_aligned.ldr==3].row,bins=[600,300],norm='symlog',cmap='Reds',alpha=0.2)\n",
    "plt.hist2d(data_aligned[data_aligned.ldr==4].col,data_aligned[data_aligned.ldr==4].row,bins=[600,300],norm='symlog',cmap='viridis',alpha=0.2,cmin=1)\n",
    "plt.axis('scaled')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering with DBSCAN (Density based )\n",
    "\n",
    "def Clustering (data) :\n",
    "    n=0\n",
    "    Cluster=[]\n",
    "    L=[]\n",
    "    m=max(data.trgNum)\n",
    "    for i in range(m+1) :\n",
    "        print('Doing clustering : ',(i*100)/m,' %',end='\\r')\n",
    "        if data[data.trgNum==i].empty ==False :\n",
    "            if (i in L) ==False :\n",
    "                L.append(i)\n",
    "                data_trgNum= data[data.trgNum==i] \n",
    "                if len(data_trgNum)!=1 :\n",
    "                    X=data_trgNum[['row','col']].to_numpy()\n",
    "                    clustering = DBSCAN(eps=1,metric='euclidean',min_samples=1).fit(X)\n",
    "                    labels = clustering.labels_ \n",
    "                    for i in labels :\n",
    "                        Cluster.append(i+n)\n",
    "                    n=max(labels)+1+n\n",
    "                else :\n",
    "                    Cluster.append(n)\n",
    "                    n+=1\n",
    "    return Cluster\n",
    "\n",
    "Clusters=[]\n",
    "for i in range (max(data_aligned.ldr)):\n",
    "    Clusters.append(Clustering(data_aligned[data_aligned.ldr==i+1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the clusters to the right data\n",
    "\n",
    "data_clusterized=data_aligned.copy()\n",
    "data_clusterized['cluster']=0\n",
    "for i in range(max(data_clusterized.ldr)):\n",
    "    data_clusterized.loc[data_clusterized.ldr==i+1,'cluster']=np.array(Clusters[i])+max(data_clusterized.cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster size\n",
    "\n",
    "def cluster_size (data) :\n",
    "    Cluster_size=pd.DataFrame({'trgNum':[], 'ldr':[],'Cluster_size':[]})\n",
    "    m=int(max(data.cluster))\n",
    "    data= data.sort_values(by=['cluster'])\n",
    "    for cluster in range(10000) :\n",
    "        print('cluster_size_percentage : ',cluster/100,end='\\r')\n",
    "        Cluster_size=pd.concat([Cluster_size,pd.DataFrame({'trgNum':[data.loc[data.cluster==cluster,'trgNum'].values[0]],'ldr':[data.loc[data.cluster==cluster,'ldr'].values[0]],'Cluster_size':[len(data[data.cluster==cluster])]})])\n",
    "    return(Cluster_size)\n",
    "\n",
    "Cluster_size=cluster_size(data_clusterized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying the distribution of the cluster size\n",
    "Cluster_size['Cluster_size'].hist(bins=20)\n",
    "plt.title('Cluster size distribution')\n",
    "plt.ylabel('Entries')\n",
    "plt.xlabel('Cluster size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster by trgNum \n",
    "Cluster_by_trgNum=[len(Cluster_size[Cluster_size.trgNum==t]) for t in range (1,int(max(Cluster_size.trgNum))+1)]\n",
    "plt.hist(Cluster_by_trgNum,bins=10)\n",
    "plt.title('Cluster by trigger Number')\n",
    "plt.ylabel('Entries')\n",
    "plt.xlable('Number of cluster by trigger number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Efficiency (4 ladders hit for a trigger)\n",
    "n=0\n",
    "for t in range (10000):\n",
    "    data_trg=data_reduced[data_reduced.trgNum==t]\n",
    "    if len(data_trg[data_trg.ldr==1])>0 and len(data_trg[data_trg.ldr==2])>0 and len(data_trg[data_trg.ldr==3])>0 and len(data_trg[data_trg.ldr==4])>0 :\n",
    "        n+=1\n",
    "print('Efficiency : ',n/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#computing barycenter\n",
    "def reduce_by_barycenter_new (data) :\n",
    "    data_sorted =data.sort_values(by=['cluster'])\n",
    "    data_reduced=pd.DataFrame({'trgNum':[],'ldr':[],'cluster':[],'Xbar':[],'Ybar':[]})\n",
    "    m=max(data_sorted.cluster)\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    n=0\n",
    "    for index, entry in data_sorted.iterrows() :\n",
    "        print('Reduction : ',(n*100/m),\" %\",end='\\r')\n",
    "        if entry.cluster == n :\n",
    "            X.append(entry.col)\n",
    "            Y.append(entry.row)\n",
    "        else :\n",
    "            e=pd.DataFrame({'trgNum':[entry.trgNum],'ldr':[entry.ldr],'cluster':[entry.cluster],'Xbar':[np.mean(X)],'Ybar':[np.mean(Y)]})\n",
    "            data_reduced=pd.concat([data_reduced,e])\n",
    "            X=[]\n",
    "            Y=[]\n",
    "            X.append(entry.col)\n",
    "            Y.append(entry.row)\n",
    "            n+=1\n",
    "    return data_reduced\n",
    "\n",
    "\n",
    "data_reduced=reduce_by_barycenter_new (data_clusterized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the data \n",
    "\n",
    "def chi2 (xexp,xfit): #computing chi2\n",
    "    chi2 =0 \n",
    "    for i in range (len(xexp)):\n",
    "        chi2+=((xexp[i]-xfit[i])**2)/xexp[i]\n",
    "    return chi2\n",
    "\n",
    "def tracking (X,Y,Z0,t) :\n",
    "    slope_X, intercept_X, r_value, p_value, std_err= stats.linregress(Z0, X)\n",
    "    slope_Y, intercept_Y, r_value, p_value, std_err= stats.linregress(Z0, Y)\n",
    "    Mean_Residuals_X=np.sqrt(np.mean((slope_X*np.array(Z0)+intercept_X-np.array(X))**2))#quadratique\n",
    "    Mean_Residuals_Y=np.sqrt(np.mean((slope_Y*np.array(Z0)+intercept_Y-np.array(Y))**2))#quadratique\n",
    "    return pd.DataFrame({'trgNum':[t],'Mean_Residuals_X':[Mean_Residuals_X],'Mean_Residuals_Y':[Mean_Residuals_Y],'slope_X':[slope_X],'intercept_X':[intercept_X],'slope_Y':[slope_Y],'intercept_Y':[intercept_Y]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Alignement from the tracking : In this more precise alignement technique we compute all the residuals from a track (3 points fitting, distance from the track of the 4th one)\n",
    "\n",
    "def precise_alignement (data,Z0) :\n",
    "    Distances_X=[[],[],[],[]]\n",
    "    Distances_Y=[[],[],[],[]]\n",
    "    m=max(data.ldr)\n",
    "    for t in range(10000) :\n",
    "        data_trgNum=data[data.trgNum==t]\n",
    "        print (\"precise_alignement_percentage : \",t/100, end='\\r')\n",
    "        if (len(data_trgNum)==m and data_trgNum.ldr.nunique() == data_trgNum.ldr.size):\n",
    "            for k in range(m):\n",
    "                X=[]\n",
    "                Y=[]\n",
    "                Z=[]\n",
    "                for j in range (m):\n",
    "                    if j!=k:\n",
    "                        X.append(data_trgNum.loc[data_trgNum.ldr==j+1,'Xbar'].values[0])\n",
    "                        Y.append(data_trgNum.loc[data_trgNum.ldr==j+1,'Ybar'].values[0])\n",
    "                        Z.append(Z0[j])\n",
    "                T=tracking (X,Y,Z,t)\n",
    "                distance_x=T.at[0,'slope_X']*Z0[k]+T.at[0,'intercept_X']-data_trgNum.loc[data_trgNum.ldr==k+1,'Xbar'].values[0]\n",
    "                distance_y=T.at[0,'slope_Y']*Z0[k]+T.at[0,'intercept_Y']-data_trgNum.loc[data_trgNum.ldr==k+1,'Ybar'].values[0]\n",
    "                Distances_X[k].append(distance_x)\n",
    "                Distances_Y[k].append(distance_y)\n",
    "    return pd.DataFrame({'dx':Distances_X,'dy':Distances_Y})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z corresponds to the original coordinates\n",
    "Z=[0,10.6,145,155.6]\n",
    "Distances=precise_alignement(data_reduced,Z)\n",
    "print('x',np.mean(Distances['dx'][0]),np.mean(Distances['dx'][1]),np.mean(Distances['dx'][2]),np.mean(Distances['dx'][3]),'Y',np.mean(Distances['dy'][0]),np.mean(Distances['dy'][1]),np.mean(Distances['dy'][2]),np.mean(Distances['dy'][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moving from the correction that we had previously\n",
    "def move (data,X,Y):\n",
    "    for i in range(max(data.ldr)) :\n",
    "        data.loc[data.ldr==i+1,'Xbar']=data[data.ldr==i+1].Xbar+np.mean(X[i])\n",
    "        data.loc[data.ldr==i+1,'Ybar']=data[data.ldr==i+1].Ybar+np.mean(Y[i])\n",
    "    return data\n",
    "\n",
    "\n",
    "data_aligned_1=data_reduced.copy()\n",
    "data_aligned_1=move(data_reduced,Distances['dx'],Distances['dy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing all the tracks for a run \n",
    "\n",
    "def make_tracks (data,Z) :\n",
    "    Tracks=pd.DataFrame({'trgNum':[],'Mean_Residuals_X':[],'Mean_Residuals_Y':[],'slope_X':[],'intercept_X':[],'slope_Y':[],'intercept_Y':[]})\n",
    "    m=int(max(data.ldr))\n",
    "    data= data.sort_values(by=['trgNum'])\n",
    "    for t in range(10000) :\n",
    "        print (\"make_tracks_percentage : \",t/100, end='\\r')\n",
    "        data_trgNum=data[data.trgNum==t]\n",
    "        if (len(data_trgNum)==m and data_trgNum.ldr.nunique() == data_trgNum.ldr.size):\n",
    "            X=[data_trgNum.loc[data_trgNum.ldr==i+1,'Xbar'].values[0] for i in range (m)]\n",
    "            Y=[data_trgNum.loc[data_trgNum.ldr==i+1,'Ybar'].values[0] for i in range (m)]\n",
    "            T=tracking (X,Y,Z,t)\n",
    "            Tracks=pd.concat([Tracks,T])\n",
    "    return Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tracks_aligned_1=make_tracks(data_aligned_1,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#event display (t: trigger number, absi: Z coordinate, ordo: X or Y coordinate)\n",
    "\n",
    "def event_display(data,track,t,absi,ordo :str,label) :\n",
    "    Absi=np.array(absi)\n",
    "    plt.title(\"event display of \" +ordo +\" in function of Z\")\n",
    "    plt.scatter(Absi,data[data.trgNum==t][ordo+'bar'])\n",
    "    plt.plot(Absi,track.loc[track.trgNum==t,'slope_'+ordo].values[0]*Absi+track.loc[track.trgNum==t,'intercept_'+ordo].values[0],label=label)\n",
    "    \n",
    "\n",
    "event_display(data_aligned_1,Tracks_aligned_1,2000,Z,'X','aligned')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean event display (the average event)\n",
    "\n",
    "def mean_event_display(data,track,Z,ordo:str,label) :\n",
    "    Z=np.array(Z)\n",
    "    plt.scatter(Z,[np.mean(data.loc[data['trgNum'].isin(np.array(track.trgNum))&(data.ldr==i+1),ordo+'bar'])for i in range(max(data.ldr))])\n",
    "    plt.plot(Z,np.mean(track['slope_'+ordo])*Z+np.mean(track['intercept_'+ordo]),label=label)\n",
    "    plt.title(ordo+'(Z) in average, points: real, line: reconstructed track',fontsize=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
